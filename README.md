# Self-hosted AI Package

**AIãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®è‡ªä½“ãƒ›ã‚¹ãƒˆ(Self-hosted AI Package)** ã‚ªãƒ¼ãƒšãƒ³ã‚½ãƒ¼ã‚¹ãªDockerComposeãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ä½ã‚³ãƒ¼ãƒ‰ã®ãƒ­ãƒ¼ã‚«ãƒ«AIã®ã‚·ã‚¹ãƒ†ãƒ ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚ã€€
ãƒ­ãƒ¼ã‚«ãƒ«AIã®å®Œäº†ã¯ã€ã€ŒOllamaã€ã®ãƒ­ãƒ¼ã‚«ãƒ«LLMs, LLMsã¨n8nã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’è©±ã™ãŸã‚ã«OpenWebUIã®ãƒãƒ£ãƒƒãƒˆã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã™ã€‚ãã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨Vectorãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨èªè¨¼ã®ãŸã‚ã«SupabaseãŒå«ã‚“ã§ã„ã¾ã™

ã“ã‚Œã¯ã€Coleã®n8nç”¨Docker Composeãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®æ—¥æœ¬èªè¨³ã§ã€ã„ãã¤ã‹ã®æ”¹å–„ã¨ã€Supabaseã€Open WebUIã€Flowiseã€SearXNGã€Caddyã®è¿½åŠ ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
ã¾ãŸã€SupabaseãŒå†…éƒ¨ã§Postgresã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€Postgresã¯å‰Šé™¤ã•ã‚Œã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€ã“ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€å‹•ç”»ã§ç´¹ä»‹ã•ã‚Œã¦ã„ã‚‹ãƒ­ãƒ¼ã‚«ãƒ«RAG AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãŒè‡ªå‹•çš„ã«n8nã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«çµ„ã¿è¾¼ã¾ã‚Œã¾ã™ã€‚

## Important Links

- [Original Local AI Starter Kit](https://github.com/n8n-io/self-hosted-ai-starter-kit) by the n8n team

- [Coleã•ã‚“ã®è‹±èªç‰ˆã®ãƒ¬ãƒ](https://github.com/coleam00/local-ai-packaged) 

- [Local AI community](https://thinktank.ottomator.ai/c/local-ai/18) forum over in the oTTomator Think Tank

- [GitHub Kanban board](https://github.com/users/coleam00/projects/2/views/1) for feature implementation and bug squashing.


- Download my N8N + OpenWebUI integration [directly on the Open WebUI site.](https://openwebui.com/f/coleam/n8n_pipe/) (more instructions below)

![n8n.io - Screenshot](https://raw.githubusercontent.com/n8n-io/self-hosted-ai-starter-kit/main/assets/n8n-demo.gif)

Curated by <https://github.com/n8n-io> and <https://github.com/coleam00>, it combines the self-hosted n8n
platform with a curated list of compatible AI products and components to
quickly get started with building self-hosted AI workflows.

### Whatâ€™s included

âœ… [**Self-hosted n8n**](https://n8n.io/) -ï¼š400ä»¥ä¸Šã®çµ±åˆæ©Ÿèƒ½ã¨é«˜åº¦ãªAIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’å‚™ãˆãŸãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚

âœ… [**Supabase**](https://supabase.com/) - ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚µãƒ¼ãƒ“ã‚¹ã§ã€AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§æœ€ã‚‚åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™

âœ… [**Ollama**](https://ollama.com/) - æœ€æ–°ã®ãƒ­ãƒ¼ã‚«ãƒ«LLMã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦å®Ÿè¡Œã§ãã‚‹ã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã®LLMãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã§ã™ã€‚



âœ… [**Open WebUI**](https://openwebui.com/) - ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ãƒ‡ãƒ«ã‚„n8nã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¨ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆã«å¯¾è©±ã§ãã‚‹ChatGPTé¢¨ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§ã™ã€‚

âœ… [**Flowise**](https://flowiseai.com/) - n8nã¨éå¸¸ã«ç›¸æ€§ã®è‰¯ã„ãƒãƒ¼ã‚³ãƒ¼ãƒ‰/ãƒ­ãƒ¼ã‚³ãƒ¼ãƒ‰ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼ã§ã™ã€‚

âœ… [**Qdrant**](https://qdrant.tech/) - åŒ…æ‹¬çš„ãªAPIã‚’å‚™ãˆãŸã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®é«˜æ€§èƒ½ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã§ã™ã€‚
RAGã®ãŸã‚ã«Supabaseã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ãŒã€Qdrantã¯Supabaseã‚ˆã‚Šã‚‚é«˜é€Ÿã§ã‚ã‚‹ãŸã‚ã€å ´åˆã«ã‚ˆã£ã¦ã¯ã‚ˆã‚Šé©ã—ãŸé¸æŠè‚¢ã¨ãªã‚Šã¾ã™ã€‚

âœ… [**SearXNG**](https://searxng.org/) - æœ€å¤§229ã®æ¤œç´¢ã‚µãƒ¼ãƒ“ã‚¹ã‹ã‚‰çµæœã‚’é›†ç´„ã™ã‚‹ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®ç„¡æ–™ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆãƒ¡ã‚¿æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¯è¿½è·¡ã‚„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã•ã‚Œãªã„ãŸã‚ã€ãƒ­ãƒ¼ã‚«ãƒ«AIãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã¨é©åˆã—ã¾ã™ã€‚

âœ… [**Caddy**](https://caddyserver.com/) - ã‚«ã‚¹ã‚¿ãƒ ãƒ‰ãƒ¡ã‚¤ãƒ³ã®ãŸã‚ã®ãƒãƒãƒ¼ã‚¸ãƒ‰HTTPS/TLSã‚’æä¾›ã—ã¾ã™ã€‚

## Prerequisites å‰ææ¡ä»¶

é–‹å§‹ã™ã‚‹å‰ã«ã€ä»¥ä¸‹ã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š

- [Python](https://www.python.org/downloads/) - ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«å¿…è¦ã§ã™ã€‚
- [Git/GitHub Desktop](https://desktop.github.com/) - ãƒªãƒã‚¸ãƒˆãƒªç®¡ç†ã‚’å®¹æ˜“ã«è¡Œã†ãŸã‚ã«ä½¿ç”¨ã—ã¾ã™ã€‚ 
- [Docker/Docker Desktop](https://www.docker.com/products/docker-desktop/) - ã™ã¹ã¦ã®ã‚µãƒ¼ãƒ“ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã«å¿…è¦ã§ã™

## Installation

ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã™ã‚‹ã‹ã€GitHub Desktopã§ã‚¯ãƒ­ãƒ¼ãƒ³ã®URLãƒªãƒ³ã‚¯ã‚’è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚:
```bash
git clone https://github.com/coleam00/local-ai-packaged.git
cd local-ai-packaged
```

ã‚µãƒ¼ãƒ“ã‚¹ã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ã€Supabaseã®ç’°å¢ƒå¤‰æ•°ã‚’ä»¥ä¸‹ã®ã‚ˆã†ã«è¨­å®šã‚’æ›´æ–°ã™ã‚‹ã“ã¨ãŒå¿…è¦ã§ã™ [self-hosting guide](https://supabase.com/docs/guides/self-hosting/docker#securing-your-services).

1. `.env.example`ã¨ã„ã†ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€ `.env`ã«æ”¹åã—ã¦ãã ã•ã„
2. ä»¥ä¸‹ã®å¿…è¦ãªç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã™ã‚‹ã€‚
   ```bash
   ############
   # N8N Configurationã€€ç’°å¢ƒå¤‰æ•°
   ############
   N8N_ENCRYPTION_KEY=
   N8N_USER_MANAGEMENT_JWT_SECRET=

   ############
   # Supabase Secretsã€€ç§˜å¯†éµ
   ############
   POSTGRES_PASSWORD=
   JWT_SECRET=
   ANON_KEY=
   SERVICE_ROLE_KEY=
   DASHBOARD_USERNAME=
   DASHBOARD_PASSWORD=
   POOLER_TENANT_ID=
   ```

> [!IMPORTANTã€€é‡è¦]
> Make sure to generate secure random values for all secrets. Never use the example values in production.

3. Set the following environment variables if deploying to production, otherwise leave commented:
   ```bash
   ############
   # Caddy Config
   ############

   N8N_HOSTNAME=n8n.yourdomain.com
   WEBUI_HOSTNAME=:openwebui.yourdomain.com
   FLOWISE_HOSTNAME=:flowise.yourdomain.com
   SUPABASE_HOSTNAME=:supabase.yourdomain.com
   OLLAMA_HOSTNAME=:ollama.yourdomain.com
   SEARXNG_HOSTNAME=searxng.yourdomain.com
   LETSENCRYPT_EMAIL=your-email-address
   ```   

---

The project includes a `start_services.py` script that handles starting both the Supabase and local AI services. The script accepts a `--profile` flag to specify which GPU configuration to use.

### For Nvidia GPU users

```bash
python start_services.py --profile gpu-nvidia
```

> [!NOTE]
> If you have not used your Nvidia GPU with Docker before, please follow the
> [Ollama Docker instructions](https://github.com/ollama/ollama/blob/main/docs/docker.md).

### For AMD GPU users on Linux

```bash
python start_services.py --profile gpu-amd
```

### For Mac / Apple Silicon users

If you're using a Mac with an M1 or newer processor, you can't expose your GPU to the Docker instance, unfortunately. There are two options in this case:

1. Run the starter kit fully on CPU:
   ```bash
   python start_services.py --profile cpu
   ```

2. Run Ollama on your Mac for faster inference, and connect to that from the n8n instance:
   ```bash
   python start_services.py --profile none
   ```

   If you want to run Ollama on your mac, check the [Ollama homepage](https://ollama.com/) for installation instructions.

#### For Mac users running OLLAMA locally

If you're running OLLAMA locally on your Mac (not in Docker), you need to modify the OLLAMA_HOST environment variable in the n8n service configuration. Update the x-n8n section in your Docker Compose file as follows:

```yaml
x-n8n: &service-n8n
  # ... other configurations ...
  environment:
    # ... other environment variables ...
    - OLLAMA_HOST=host.docker.internal:11434
```

Additionally, after you see "Editor is now accessible via: http://localhost:5678/":

1. Head to http://localhost:5678/home/credentials
2. Click on "Local Ollama service"
3. Change the base URL to "http://host.docker.internal:11434/"

### For everyone else

```bash
python start_services.py --profile cpu
```

## Deploying to the Cloud

### Prerequisites for the below steps

- Linux machine (preferably Unbuntu) with Nano, Git, and Docker installed

### Extra steps

Before running the above commands to pull the repo and install everything:

1. Run the commands as root to open up the necessary ports:
   - ufw enable
   - ufw allow 8000 && ufw allow 3001 && ufw allow 3000 && ufw allow 5678 && ufw allow 80 && ufw allow 443
   - ufw allow 8080 (if you want to expose SearXNG)
   - ufw allow 11434 (if you want to expose Ollama)
   - ufw reload

2. Set up A records for your DNS provider to point your subdomains you'll set up in the .env file for Caddy
to the IP address of your cloud instance.

   For example, A record to point n8n to [cloud instance IP] for n8n.yourdomain.com

## âš¡ï¸ Quick start and usage

The main component of the self-hosted AI starter kit is a docker compose file
pre-configured with network and disk so there isnâ€™t much else you need to
install. After completing the installation steps above, follow the steps below
to get started.

1. Open <http://localhost:5678/> in your browser to set up n8n. Youâ€™ll only
   have to do this once. You are NOT creating an account with n8n in the setup here,
   it is only a local account for your instance!
2. Open the included workflow:
   <http://localhost:5678/workflow/vTN9y2dLXqTiDfPT>
3. Create credentials for every service:
   
   Ollama URL: http://ollama:11434

   Postgres (through Supabase): use DB, username, and password from .env. IMPORTANT: Host is 'db'
   Since that is the name of the service running Supabase

   Qdrant URL: http://qdrant:6333 (API key can be whatever since this is running locally)

   Google Drive: Follow [this guide from n8n](https://docs.n8n.io/integrations/builtin/credentials/google/).
   Don't use localhost for the redirect URI, just use another domain you have, it will still work!
   Alternatively, you can set up [local file triggers](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.localfiletrigger/).
4. Select **Test workflow** to start running the workflow.
5. If this is the first time youâ€™re running the workflow, you may need to wait
   until Ollama finishes downloading Llama3.1. You can inspect the docker
   console logs to check on the progress.
6. Make sure to toggle the workflow as active and copy the "Production" webhook URL!
7. Open <http://localhost:3000/> in your browser to set up Open WebUI.
Youâ€™ll only have to do this once. You are NOT creating an account with Open WebUI in the 
setup here, it is only a local account for your instance!
8. Go to Workspace -> Functions -> Add Function -> Give name + description then paste in
the code from `n8n_pipe.py`

   The function is also [published here on Open WebUI's site](https://openwebui.com/f/coleam/n8n_pipe/).

9. Click on the gear icon and set the n8n_url to the production URL for the webhook
you copied in a previous step.
10. Toggle the function on and now it will be available in your model dropdown in the top left! 

To open n8n at any time, visit <http://localhost:5678/> in your browser.
To open Open WebUI at any time, visit <http://localhost:3000/>.

With your n8n instance, youâ€™ll have access to over 400 integrations and a
suite of basic and advanced AI nodes such as
[AI Agent](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.agent/),
[Text classifier](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.text-classifier/),
and [Information Extractor](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.information-extractor/)
nodes. To keep everything local, just remember to use the Ollama node for your
language model and Qdrant as your vector store.

> [!NOTE]
> This starter kit is designed to help you get started with self-hosted AI
> workflows. While itâ€™s not fully optimized for production environments, it
> combines robust components that work well together for proof-of-concept
> projects. You can customize it to meet your specific needs

## Upgrading

To update all containers to their latest versions (n8n, Open WebUI, etc.), run these commands:

```bash
# Stop all services
docker compose -p localai -f docker-compose.yml -f supabase/docker/docker-compose.yml down

# Pull latest versions of all containers
docker compose -p localai -f docker-compose.yml -f supabase/docker/docker-compose.yml pull

# Start services again with your desired profile
python start_services.py --profile <your-profile>
```

Replace `<your-profile>` with one of: `cpu`, `gpu-nvidia`, `gpu-amd`, or `none`.

Note: The `start_services.py` script itself does not update containers - it only restarts them or pulls them if you are downloading these containers for the first time. To get the latest versions, you must explicitly run the commands above.

## Troubleshooting

Here are solutions to common issues you might encounter:

### Supabase Issues

- **Supabase Pooler Restarting**: If the supabase-pooler container keeps restarting itself, follow the instructions in [this GitHub issue](https://github.com/supabase/supabase/issues/30210#issuecomment-2456955578).

- **Supabase Analytics Startup Failure**: If the supabase-analytics container fails to start after changing your Postgres password, delete the folder `supabase/docker/volumes/db/data`.

- **If using Docker Desktop**: Go into the Docker settings and make sure "Expose daemon on tcp://localhost:2375 without TLS" is turned on

- **Supabase Service Unavailable** - Make sure you don't have an "@" character in your Postgres password! If the connection to the kong container is working (the container logs say it is receiving requests from n8n) but n8n says it cannot connect, this is generally the problem from what the community has shared. Other characters might not be allowed too, the @ symbol is just the one I know for sure!

### GPU Support Issues

- **Windows GPU Support**: If you're having trouble running Ollama with GPU support on Windows with Docker Desktop:
  1. Open Docker Desktop settings
  2. Enable WSL 2 backend
  3. See the [Docker GPU documentation](https://docs.docker.com/desktop/features/gpu/) for more details

- **Linux GPU Support**: If you're having trouble running Ollama with GPU support on Linux, follow the [Ollama Docker instructions](https://github.com/ollama/ollama/blob/main/docs/docker.md).

## ğŸ‘“ Recommended reading

n8n is full of useful content for getting started quickly with its AI concepts
and nodes. If you run into an issue, go to [support](#support).

- [AI agents for developers: from theory to practice with n8n](https://blog.n8n.io/ai-agents/)
- [Tutorial: Build an AI workflow in n8n](https://docs.n8n.io/advanced-ai/intro-tutorial/)
- [Langchain Concepts in n8n](https://docs.n8n.io/advanced-ai/langchain/langchain-n8n/)
- [Demonstration of key differences between agents and chains](https://docs.n8n.io/advanced-ai/examples/agent-chain-comparison/)
- [What are vector databases?](https://docs.n8n.io/advanced-ai/examples/understand-vector-databases/)

## ğŸ¥ Video walkthrough

- [Cole's Guide to the Local AI Starter Kit](https://youtu.be/pOsO40HSbOo)

## ğŸ›ï¸ More AI templates

For more AI workflow ideas, visit the [**official n8n AI template
gallery**](https://n8n.io/workflows/?categories=AI). From each workflow,
select the **Use workflow** button to automatically import the workflow into
your local n8n instance.

### Learn AI key concepts

- [AI Agent Chat](https://n8n.io/workflows/1954-ai-agent-chat/)
- [AI chat with any data source (using the n8n workflow too)](https://n8n.io/workflows/2026-ai-chat-with-any-data-source-using-the-n8n-workflow-tool/)
- [Chat with OpenAI Assistant (by adding a memory)](https://n8n.io/workflows/2098-chat-with-openai-assistant-by-adding-a-memory/)
- [Use an open-source LLM (via HuggingFace)](https://n8n.io/workflows/1980-use-an-open-source-llm-via-huggingface/)
- [Chat with PDF docs using AI (quoting sources)](https://n8n.io/workflows/2165-chat-with-pdf-docs-using-ai-quoting-sources/)
- [AI agent that can scrape webpages](https://n8n.io/workflows/2006-ai-agent-that-can-scrape-webpages/)

### Local AI templates

- [Tax Code Assistant](https://n8n.io/workflows/2341-build-a-tax-code-assistant-with-qdrant-mistralai-and-openai/)
- [Breakdown Documents into Study Notes with MistralAI and Qdrant](https://n8n.io/workflows/2339-breakdown-documents-into-study-notes-using-templating-mistralai-and-qdrant/)
- [Financial Documents Assistant using Qdrant and](https://n8n.io/workflows/2335-build-a-financial-documents-assistant-using-qdrant-and-mistralai/)Â [Â Mistral.ai](http://mistral.ai/)
- [Recipe Recommendations with Qdrant and Mistral](https://n8n.io/workflows/2333-recipe-recommendations-with-qdrant-and-mistral/)

## Tips & tricks

### Accessing local files

The self-hosted AI starter kit will create a shared folder (by default,
located in the same directory) which is mounted to the n8n container and
allows n8n to access files on disk. This folder within the n8n container is
located at `/data/shared` -- this is the path youâ€™ll need to use in nodes that
interact with the local filesystem.

**Nodes that interact with the local filesystem**

- [Read/Write Files from Disk](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.filesreadwrite/)
- [Local File Trigger](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.localfiletrigger/)
- [Execute Command](https://docs.n8n.io/integrations/builtin/core-nodes/n8n-nodes-base.executecommand/)

## ğŸ“œÂ License

This project (originally created by the n8n team, link at the top of the README) is licensed under the Apache License 2.0 - see the
[LICENSE](LICENSE) file for details.
